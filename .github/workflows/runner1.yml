name: "runner1"

on:
  schedule:
    - cron: '0 13 * * *'
  workflow_dispatch:
    inputs:
      simulate_schedule:
        description: 'Set to true to simulate schedule behaviour (enable jitter) for manual runs)'
        required: false
        default: 'false'

jobs:
  run-rewards:
    runs-on: ubuntu-latest
    timeout-minutes: 360
    env:
      NODE_VERSION: '20'
      SIMULATE_SCHEDULE: ${{ github.event.inputs.simulate_schedule || 'false' }}
      JOB_INDEX: '0'
      GROUP_SIZES: '3,2,2'
      TARGET_HOUR_TORONTO: '9'
      MAX_JITTER_SECONDS: '1500'
      OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
      # -------------------------
      # Shadowsocks: install + configure oracle.json (manual steps version)
      # -------------------------
      - name: Install and configure Shadowsocks (following manual steps)
        env:
          SHADOWSOCKS_SERVER: ${{ secrets.SHADOWSOCKS_SERVER }}
          SHADOWSOCKS_PASSWORD: ${{ secrets.SHADOWSOCKS_PASSWORD }}
          SHADOWSOCKS_LOCAL_ADDRESS: ${{ secrets.SHADOWSOCKS_LOCAL_ADDRESS || '127.0.0.1' }}
        shell: bash
        run: |
          set -euo pipefail

          echo "Step 1: Installing shadowsocks-libev..."
          sudo apt-get update -y
          DEBIAN_FRONTEND=noninteractive sudo apt-get install -y shadowsocks-libev

          echo "Step 2: Checking initial service status..."
          sudo systemctl status shadowsocks-libev || true

          echo "Step 3: Disabling shadowsocks-libev service..."
          sudo systemctl disable shadowsocks-libev --now || true

          echo "Step 4: Configuring /etc/shadowsocks-libev directory..."
          cd /etc/shadowsocks-libev/

          if [ -f config.json ]; then
            sudo mv config.json config.json.default
            echo "Backed up existing config.json to config.json.default"
          else
            echo "No existing config.json found, creating default backup..."
            sudo touch config.json.default
          fi

          echo "Step 5: Creating oracle.json..."
          if [ -f config.json.default ]; then
            echo "Default config content (for reference):"
            sudo cat config.json.default | head -20 || true
          fi

          sudo tee oracle.json > /dev/null <<EOF
          {
            "server": "$SHADOWSOCKS_SERVER",
            "server_port": 8388,
            "local_address": "$SHADOWSOCKS_LOCAL_ADDRESS",
            "local_port": 1080,
            "password": "$SHADOWSOCKS_PASSWORD",
            "timeout": 300,
            "method": "chacha20-ietf-poly1305"
          }
          EOF

          echo "Step 6: oracle.json content:"

          echo "Step 7: Enabling and starting shadowsocks-libev-local@oracle..."
          sudo systemctl enable shadowsocks-libev-local@oracle --now

          echo "Step 8: Checking service status..."
          systemctl status shadowsocks-libev-local@oracle

          echo "Step 9: Checking list status..."
          sudo lsof -P -i -n

          sleep 3
          if sudo systemctl is-active shadowsocks-libev-local@oracle; then
            echo "✅ Shadowsocks oracle service is running successfully!"
          else
            echo "❌ Service failed to start. Checking logs..."
            sudo journalctl -u shadowsocks-libev-local@oracle -n 20 --no-pager
            exit 1
          fi

      # -------------------------
      # End shadowsocks block
      # -------------------------

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: package-lock.json

      - name: Decide whether this scheduled run should continue (Toronto gate)
        shell: bash
        run: |
          set -euo pipefail
          EVENT_NAME="${GITHUB_EVENT_NAME:-}"
          if [ "$EVENT_NAME" = "workflow_dispatch" ] && [ "${SIMULATE_SCHEDULE:-}" != "true" ]; then
            echo "Manual run requested (no hour check)."
          else
            LOCAL_HOUR=$(TZ='America/Toronto' date +%H | sed 's/^0//')
            echo "Toronto local hour: ${LOCAL_HOUR}"
            if [ "$LOCAL_HOUR" -ne "${TARGET_HOUR_TORONTO}" ]; then
              echo "Local hour is not ${TARGET_HOUR_TORONTO} — exiting."
              exit 0
            fi
            echo "Local hour matches target (${TARGET_HOUR_TORONTO}) — continuing."
          fi

      - name: Compute random jitter (0..MAX_JITTER_SECONDS)
        shell: bash
        run: |
          set -euo pipefail
          if [ "${GITHUB_EVENT_NAME:-}" = "workflow_dispatch" ] && [ "${SIMULATE_SCHEDULE:-}" != "true" ]; then
            echo "Manual run — skipping jitter."
            JITTER_SEC=0
          else
            MAX=${MAX_JITTER_SECONDS:-1500}
            JITTER_SEC=$(( $(od -An -N4 -tu4 /dev/urandom | tr -d ' ') % (MAX + 1) ))
            echo "Sleeping $JITTER_SEC seconds (jitter up to $MAX)."
            sleep "$JITTER_SEC"
          fi

      - name: Write src/accounts.json from secret (robust + locked perms)
        shell: bash
        env:
          ACCOUNTS_JSON_B64: ${{ secrets.ACCOUNTS_JSON_B64 || '' }}
          ACCOUNTS_JSON: ${{ secrets.ACCOUNTS_JSON || '' }}
        run: |
          set -euo pipefail
          # strict permissions for any created files
          umask 077
          mkdir -p src dist

          echo "=== repo root listing (masked) ==="
          # show repo listing but hide any accounts.json to avoid accidental exposure
          ls -la | sed '/src\/accounts.json/d' || true

          TMP=""
          trap '[[ -n "$TMP" && -f "$TMP" ]] && rm -f "$TMP" || true' EXIT

          if [ -n "${ACCOUNTS_JSON_B64:-}" ]; then
            echo "Decoding ACCOUNTS_JSON_B64 -> src/accounts.json"
            if printf '%s' "$ACCOUNTS_JSON_B64" | base64 --decode > src/accounts.json 2>/dev/null; then
              echo "Decoded to src/accounts.json"
            else
              echo "ERROR decoding ACCOUNTS_JSON_B64" >&2
              exit 1
            fi
          elif [ -n "${ACCOUNTS_JSON:-}" ]; then
            RAW="${ACCOUNTS_JSON}"
            case "$RAW" in
              \"*\" ) RAW="${RAW#\"}"; RAW="${RAW%\"}" ;;
              \'*\' ) RAW="${RAW#\'}"; RAW="${RAW%\'}" ;;
            esac
            case "$RAW" in
              [\{[]* )
                printf '%s' "$RAW" > src/accounts.json
                echo "Wrote raw JSON to src/accounts.json"
                ;;
              * )
                TMP=$(mktemp)
                if printf '%s' "$RAW" | base64 --decode > "$TMP" 2>/dev/null; then
                  if command -v jq >/dev/null 2>&1; then
                    if jq . "$TMP" >/dev/null 2>&1; then
                      mv "$TMP" src/accounts.json
                      TMP=""
                      echo "Decoded base64 -> src/accounts.json (validated jq)"
                    else
                      rm -f "$TMP"
                      TMP=""
                      echo "ERROR: decoded content not valid JSON" >&2
                      exit 1
                    fi
                  else
                    if node -e "try{JSON.parse(require('fs').readFileSync('$TMP','utf8')); process.exit(0)}catch(e){ process.exit(2) }"; then
                      mv "$TMP" src/accounts.json
                      TMP=""
                      echo "Decoded base64 -> src/accounts.json (validated node)"
                    else
                      rm -f "$TMP"
                      TMP=""
                      echo "ERROR: decoded content not valid JSON (node)" >&2
                      exit 1
                    fi
                  fi
                else
                  rm -f "$TMP" 2>/dev/null || true
                  TMP=""
                  echo "ERROR: ACCOUNTS_JSON not raw JSON and not valid base64" >&2
                  exit 1
                fi
                ;;
            esac
          else
            echo "No ACCOUNTS_JSON provided; attempting example fallback"
            if [ -f src/accounts.example.json ]; then
              cp src/accounts.example.json src/accounts.json
            elif [ -f accounts.example.json ]; then
              cp accounts.example.json src/accounts.json
            else
              echo "ERROR: No accounts secret and no example file" >&2
              ls -la || true
              exit 1
            fi
          fi

          # Validate JSON without printing contents
          if command -v jq >/dev/null 2>&1; then
            jq . src/accounts.json >/dev/null || { echo "Invalid JSON in src/accounts.json" >&2; exit 1; }
            echo "src/accounts.json validated by jq"
          else
            node -e "try{JSON.parse(require('fs').readFileSync('src/accounts.json','utf8')); console.log('validated')}catch(e){ console.error('Invalid JSON:', e.message); process.exit(2) }"
          fi

          # copy to dist immediately (build/runtime needs it) and lock permissions
          cp src/accounts.json dist/accounts.json
          chmod 600 src/accounts.json dist/accounts.json || true
          echo "Wrote and locked src/accounts.json and dist/accounts.json (owner-only)."

      - name: Propagate slice, mask summary (so build & runtime use the same accounts but we never print secrets)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p dist
          cat > dist/mask_and_copy.js <<'NODE'
          const fs = require('fs');
          const src = 'src/accounts.json';
          if (!fs.existsSync(src)) { console.error('src/accounts.json not found'); process.exit(1); }
          const arr = JSON.parse(fs.readFileSync(src,'utf8') || '[]');

          // normalize/write back to src (no-op) to ensure consistent formatting
          fs.writeFileSync('src/accounts.json', JSON.stringify(arr, null, 2));

          function maskEmail(email){
            if(!email || typeof email !== 'string') return '(no-email)';
            const [local, domain] = email.split('@');
            if(!domain) return '***REDACTED***';
            const first = local.slice(0,2);
            const last = local.slice(-1);
            const middleMask = local.length <= 3 ? '***' : '****';
            return `${first}${middleMask}${last}@${domain}`;
          }

          const summary = arr.map((a, idx) => {
            const email = a.email || a.username || a.user || '';
            const id = a.id || a.name || a.username || null;
            return { index: idx, id: id || null, email: email ? maskEmail(email) : '(no-email)' };
          });

          console.log('Assigned account count:', summary.length);
          console.log('Masked accounts (first 10):', JSON.stringify(summary.slice(0,10), null, 2));

          if (!fs.existsSync('dist')) fs.mkdirSync('dist', { recursive: true });
          fs.writeFileSync('dist/assigned_masked_summary.json', JSON.stringify(summary, null, 2));
          NODE

          node dist/mask_and_copy.js
          chmod 600 dist/assigned_masked_summary.json || true

      - name: Upload assigned masked summary for debug
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: assigned-accounts-masked-job${{ env.JOB_INDEX }}-${{ github.run_id }}
          path: dist/assigned_masked_summary.json

      - name: Install Node dependencies (npm ci)
        run: npm ci --no-audit --no-fund

      - name: Cache Playwright browser binaries
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: playwright-browser-cache-${{ runner.os }}-${{ hashFiles('package-lock.json') }}
          restore-keys: |
            playwright-browser-cache-${{ runner.os }}-

      - name: Install Playwright Chromium + OS dependencies
        shell: bash
        run: |
          set -euo pipefail
          npx --yes playwright install --with-deps chromium

      - name: Install xvfb (virtual X server)
        shell: bash
        run: |
          set -eux
          sudo apt-get update
          sudo apt-get install -y xvfb
          dpkg -l | head -n 20

      - name: Build the project
        run: npm run build

      - name: Ensure dist/accounts.json exists (copy after build)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p dist
          if [ -f dist/accounts.json ]; then
            echo "dist/accounts.json present"
          elif [ -f src/accounts.json ]; then
            cp src/accounts.json dist/accounts.json
            chmod 600 dist/accounts.json || true
            echo "Copied src/accounts.json -> dist/accounts.json"
          else
            echo "WARNING: src/accounts.json missing — job may fail." >&2
            ls -la src || true
          fi

      - name: Create runtime File polyfill (fallback)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p dist
          cat > dist/polyfill-file.js <<'EOF'
          if (typeof globalThis.File === 'undefined') {
            globalThis.File = class File extends Blob {
              constructor(parts = [], name = '', options = {}) {
                super(parts, options);
                this.name = String(name);
                this.lastModified = options && options.lastModified ? Number(options.lastModified) : Date.now();
              }
            };
          }
          EOF

      - name: Run the built script under Xvfb and save logs
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p logs
          # run under Xvfb; capture stdout/stderr into logs/script.log
          xvfb-run --auto-servernum --server-args='-screen 0 1280x1024x24' \
            sh -c "stdbuf -oL -eL node -r ./dist/polyfill-file.js ./dist/index.js 2>&1 | tee -a logs/script.log" || true
          ps auxww > logs/ps-auxww.txt || true
          ls -laR > logs/ls-laR.txt || true
          # IMPORTANT: do NOT dump the entire environment to a file (would leak secrets)
          # env > logs/env.raw || true   <-- removed to prevent secret leakage

      - name: Sanitize logs (mask emails, redact tokens & passwords) before upload
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p dist logs
          cat > dist/sanitize_logs.js <<'NODE'
          const fs = require('fs');
          const { execSync } = require('child_process');

          const logPath = 'logs/script.log';
          const filesToSanitize = [logPath].filter(p => fs.existsSync(p));

          function maskEmailStr(s){
            return s.replace(/([A-Za-z0-9._%+-]+)@([A-Za-z0-9.-]+\.[A-Za-z]{2,})/g, (m, local, domain) => {
              const first = local.slice(0,2);
              const last = local.slice(-1);
              const middleMask = local.length <= 3 ? '***' : '****';
              return `${first}${middleMask}${last}@${domain}`;
            });
          }

          function sanitizeContent(content){
            if(!content) return content;
            // Truncate extremely long lines (protect against huge tokens)
            content = content.split('\\n').map(line => line.length > 1000 ? line.slice(0,1000) + '...[TRUNCATED]' : line).join('\\n');

            // mask emails
            content = maskEmailStr(content);

            // redact JSON-style secret fields: "password": "...", "token": "...", "secret": "..."
            content = content.replace(/("(?:(?:P|p)assword|pwd|pass|token|secret|api[_-]?key|apikey)\"?\\s*:\\s*\")([^"]+)\"/g, '$1***REDACTED***"');

            // redact shell-style assignments like PASSWORD=..., TOKEN=...
            content = content.replace(/(\\b(?:(?:P|p)ass(?:word)?|pwd|token|secret|api[_-]?key|apikey|auth|key|credential)\\b\\s*[:=]\\s*)([^\\s,;]+)/gi, '$1***REDACTED***');

            // redact Authorization Bearer/Basic tokens
            content = content.replace(/(Authorization[:=]?\\s*)(Bearer|Basic)?\\s*[A-Za-z0-9\\-_.=]+/gi, '$1$2 ***REDACTED***');

            // redact long base64/hex/blobs
            content = content.replace(/\\b[A-Za-z0-9+\\/]{40,}={0,2}\\b/g, '***REDACTED_BASE64***');
            content = content.replace(/\\b[0-9a-fA-F]{40,}\\b/g, '***REDACTED_HEX***');

            // collapse sequences that look like JWTs
            content = content.replace(/[A-Za-z0-9-_]{10,}\\.[A-Za-z0-9-_]{10,}\\.[A-Za-z0-9-_]{10,}/g, '***REDACTED_JWT***');

            return content;
          }

          for (const p of filesToSanitize) {
            try {
              const raw = fs.readFileSync(p, 'utf8');
              fs.writeFileSync(p, sanitizeContent(raw), 'utf8');
            } catch (e) {
              // ignore read/write errors
            }
          }

          // lock down logs and dist to owner-only where possible
          try { execSync('chmod -R 600 logs || true'); } catch(e) {}
          try { execSync('chmod -R 600 dist || true'); } catch(e) {}

          // create a sanitized tarball for upload (contains sanitized logs)
          try { execSync('tar -czf logs-full-sanitized.tar.gz logs || true'); } catch(e) {}

          console.log('Sanitization complete. Files locked to owner-only where possible.');
          NODE

          node dist/sanitize_logs.js || true

          # additional fallback redaction (defensive)
          sed -E -i 's/("(?:(?:P|p)assword|pwd|pass|token|secret)\"?\\s*:\\s*\")[^\"]+\"/\\1***REDACTED***\"/Ig' logs/script.log || true
          sed -E -i 's/([Pp]ass(word)?|pwd|token|secret|api[_-]?key|apikey|auth|key|credential)[[:space:]]*[:=][[:space:]]*[^[:space:],;]+/\\1=***REDACTED***/g' logs/script.log || true
          sed -E -i 's/[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}/***REDACTED_EMAIL***/g' logs/script.log || true
          chmod -R 600 logs dist || true
